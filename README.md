# odtp

This tools allows you to: 

- Serve an user interface to manage and run your digital twins. 
- Execute ODTP components.
- Check digital twins iterations.
- Check running container logs.
- Design and run digital twins using a node base workflow tool.
- Restart and test different schemas for MongoDB / S3.
- Check outputs/snapshots and download results. 


## How to use it?

1. Download the repository. 
2. Run `poetry install .`
3. Run `odtp --help`

### How to run a single component?

In this example we are going to run ()[ODTP component example]. First, we will prepare the component which will automatically download the repostory, build the image and prepare all the folders needed for the input / output data. 

First let's create a project folder called `digital_twin_project`. In this folder is where all the folders will appear. 

```
mkdir digital_twin_project
```
 
 Then we can prepare the project by running the following. This will download the repo and build the image. 

 ```
 odtp component prepare --folder /Users/carlosvivarrios/pro/odtp/digital_twin_project --image_name image_test --repository https://github.com/odtp-org/odtp-component-example
 ```

 Now we need to run the component: 

 ```
 odtp component run --folder /Users/carlosvivarrios/pro/odtp/digital_twin_project --image_name image_test --repository https://github.com/odtp-org/odtp-component-example --env_file /Users/carlosvivarrios/pro/odtp/digital_twin_project/.env --instance_name instance_test
 ```

Then we can delete the instance by running. In docker terminology this will remove the container

```
odtp component delete-instance --instance_name instance_test
```

And finally if we want to delete the image we can run:

```
odtp component delete-image --image_name image_test 
```

### How to add  a new user? 

### How to index a new component?

### How to create a new digital twin? 

### How to create a new execution of a digital twin?

### How to run the GUI dashboard?

The dashboard functionality is limited right now and still require an update to the version v0.2.0. However it can be deployed by going to the repository folder and running: `odtp dashboard run --port 8501`



## Concept

The idea of odtp is to be installed as an instance in small-medium computing platform (such a servers, workstations, laptops, etc).  

The arquitecture of the odtp include different core-modules dealing with specific task. Between parenthesis you can find the technologies that are being considered for this modules.

- GUI (Streamlit)
- CLI
- Authentication (eduID, GH)
- Workflow manager (Barfi)
- ODTP orchestrator (ODTP original)
- License manager (Swiss Data Custodian) #core-optional
- Data governance (Swiss Data Custodian) #core-optional
- Semantic validator engine (TopBrains) #core-optional
- KG/Ontology storing (GraphDB) #core-optional
- Snapshots/Data transferring (MINION S3)
- Performance Logging (Grafana) 

All these core modules will be available in the full instance. However, for those users who wants to try a lighter version they can omit the core-optional modules having only the following configuration.

- Core Modules
    - GUI (Streamlit)
    - CLI
    - Authentication (eduID, GH)
    - Workflow manager (Barfi)
    - ODTP orchestrator (ODTP original)
    - Traces/Logging/Users data storing (MongoDB)
    - Snapshots/Data transferring (MINION S3)
    - Performance Logging (Grafana) 

- Core-Optional Modules
    - Semantic validator engine (TopBrains) #core-optional
    - KG/Ontology storing (GraphDB) #core-optional
    - License manager (Swiss Data Custodian) #core-optional
    - Data governance (Swiss Data Custodian) #core-optional

Finally the ODTP will be complemented with a components zoo that will include extensions of 3 types:

- X number of dataloaders.
- Y number of analytical components.
- Z number of visualization components.

## Changelog

- v.0.2.0: Improvements in database and files management.
    - New MongoDB Schema. (Here)
    - ODTP digital Zoo compatibility. 
    - Automatic generation/deletion of initial volumes for docker. 
    - S3 extended functionality.


- v.0.1.0: Basic UI
    - Streamlit APP with different
    - User tagging
    - Component listing placeholder
    - Digital Twins listing
    - Snapshots listing.
    - Workflow desginer.
    - pygwalker data visualization.
    - MongoDB is required to be deployed independtly.
    - S3 is required to be deployed indepently.

## Technologies involved

- Streamlit (UI)
- Barfi (Worflow manager)
- MongoDB (Document Database)
- S3 (Storage Sytem)
- Docker (Container Technology)
- pygwalker (Data Visualization)

## Terminology

- ODTP: A tool designed to manage, run, and design digital twins. It offers an interface (CLI, and GUI) for running and managing digital twins. It wraps different open source technologies to provide a high level API for the final user. 
- Components (ODTP Term): Extensions generated by us or the community that perform specific tasks in the digital twin. The input/output is validated semantically, and they run within a docker container as an independent micro-service. They can be one of the following categories:
	- Dataloader component. 
	- Analytical component.
	- Visualization component.
- Core/core-optional modules (ODTP Term): These modules are the different parts that we are developing for the ODTP. These core modules include the different classes/methods needed to run the tool and wrap the services used. Some of these modules are not mandatory in order to run ODTP with the minimal features (i.e. running manually odtp components).
- Services: One service or micro-service, in a micro-services architecture refers to one logical unit that performs one specific task in an independent manner. In ODTP we use different servers to support core modules, such as MongoDB for the database, Minion for the storage, or GraphDB for the knowledge graph storing. But also, from a technical standpoint every component is turned into a micro-service when running. I think this is the part thatâ€™s bringing more confusion. 

## How to install ODTP

In order to install ODTP v0.2.0 we recommend to use [https://python-poetry.org/](https://python-poetry.org/). 

After you installed it, go to the repository folder and run `poetry install .`

After that you should be able to run commands using the CLI: `odtp`

## Documentation 

### How to deploy a mongoDB

```
docker run --name mongodb-instance -it -v /home/[USER]/mongodb:/data/db -e MONGO_INITDB_ROOT_USERNAME=[USER] -e MONGO_INITDB_ROOT_PASSWORD=[PASS] -e MONGO_INITDB_DATABASE=odtp -p 27017:27017 mongo:latest
```

### MongoDB Schema

This is the schema for odtp database. 

v.0.2.0 Schema

```python
# Users Collection
users = {
    "_id": ObjectId(),
    "displayName": "John Doe",
    "email": "john@example.com",
    "github": "johnDoeRepo",
    "created_at": datetime.utcnow(),
    "updated_at": datetime.utcnow(),
}

# Components Collection
components = {
    "_id": ObjectId(),
    "author": "Test",
    "componentName": "ComponentX",
    "status": "active",
    "title": "Title for ComponentX",
    "description": "Description for ComponentX",
    "tags": ["tag1", "tag2"],
    "created_at": datetime.utcnow(),
    "updated_at": datetime.utcnow(),
}

# Versions Collection
versions = {
    "_id": ObjectId(),
    "componentId": ObjectId(),
    "version": "v1.0",
    "component_version": "1.0.0",
    "repoLink": "https://github.com/...",
    "dockerHubLink": "https://hub.docker.com/...",
    "parameters": {},
    "title": "Title for Version v1.0",
    "description": "Description for Version v1.0",
    "tags": ["tag1", "tag2"],
    "created_at": datetime.utcnow(),
    "updated_at": datetime.utcnow(),
}

# DigitalTwins Collection
digitalTwins = {
    "_id": ObjectId(),
    "userRef": ObjectId(),
    "name" : "title",
    "status": "active",
    "public": True,
    "created_at": datetime.utcnow(),
    "updated_at": datetime.utcnow(),
    "executions": [ObjectId()]  # Array of ObjectIds referencing Executions collection
}

# Executions
executions = {
    "_id": ObjectId(),
    "digitalTwinRef": ObjectId(),  # Reference to DigitalTwins collection
    "title": "Title for Execution",
    "description": "Description for Execution",
    "tags": ["tag1", "tag2"],
    "workflowSchema": {
        "workflowExecutor": "barfi",
        "worflowExecutorVersion": "v2.0",
        "components": [{"component": ObjectId(),
                        "version": ObjectId() }],  # Array of ObjectIds for components
        "WorkflowExecutorSchema": {}
    },
    "start_timestamp": datetime.utcnow(),
    "end_timestamp": datetime.utcnow(),
    "steps": [ObjectId()]  # Array of ObjectIds referencing Steps collection. Change in a future by DAG graph.
}

# Steps
steps = {
    "_id": ObjectId(),
    "executionRef": ObjectId(),  # Reference to Executions collection
    "timestamp": datetime.utcnow(),
    "start_timestamp": datetime.utcnow(),
    "end_timestamp": datetime.utcnow(),
    "type": "interactive" or "ephemeral",
    "logs": [{
        "timestamp": datetime.utcnow(),
        "type": "DEBUG",
        "logstring": "Test log"
    }],
    "inputs": {},
    "outputs": {},
    "component": ObjectId(),
    "component_version": ObjectId(),
    "parameters": {},
    "output": ObjectId()
}

output = {
    "_id": ObjectId(),
    "stepRef": ObjectId(),  # Reference to the Step this output is associated with
    "output_type": "snapshot" or "output",
    "s3_bucket": "bucket_name",  # Name of the S3 bucket where the output is stored
    "s3_key": "path/to/output",  # The key (path) in the S3 bucket to the output
    "file_name": "output_file_name",  # The name of the file in the output
    "file_size": 123456,  # Size of the file in bytes
    "file_type": "image/jpeg",  # MIME type or file type
    "created_at": datetime.utcnow(),  # Timestamp when the output was created
    "updated_at": datetime.utcnow(),  # Timestamp when the output was last updated
    "metadata": {  # Additional metadata associated with the output
        "description": "Description of the output",
        "tags": ["tag1", "tag2"],
        "other_info": "Other relevant information"
    },
    "access_control": {  # Information about who can access this output
        "public": False,  # Indicates if the output is public or private
        "authorized_users": [ObjectId()],  # Array of User ObjectIds who have access
    }
}


# Results Collection
results = {
    "_id": ObjectId(),
    "executionRef": ObjectId(),
    "digitalTwinRef": ObjectId(),  # Direct reference to the DigitalTwin
    "output": [ObjectId()],
    "title": "Title for Result",
    "description": "Description for Result",
    "tags": ["tag1", "tag2"],
    "created_at": datetime.utcnow(),
    "updated_at": datetime.utcnow(),
}
```
### How to request an EPFL S3 Instance

TO BE DONE

## Development.

Developed by SDSC/CSFM